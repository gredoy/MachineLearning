---
title: "Machine Learning Write Up"
author: "Gredy Garrido"
output: html_document
---


# Executive Summary
In this project, we predict 6 classes of variables using the Random Forest package.  Random subjects were asked to correctly preform a unilateral dumbbell biceps curls then also asked to preform the same exercise incorrectly in four different ways.  The subjects wore sensors on different parts of their bodies as well as on the dumbbells they were lifting.

The overall out-of-sample error was 0.41%.

The classes we are predicting consist of the following:

- A: Correct unilateral dumbbell biceps curl
- B: Throwing the elbows to the front
- C: Lifting the dumbbell only halfway
- D: Lowering the dumbbell only halfway
- E: Throwing the hips to the front


# Methodology
The initial data consisted of 160 variables but most of which contained 98% missing or NA values as well as personal identification variables that we weren't interested in.  Hence, we decided to simply drop these variables from both the training and test data.  We partitioned the initial data into training and test data sets.  We then used the final "testing" data as a validation for submission on the course website.  We got all the cases correct.

We built multiple models with different methodologies but only choose Random Forest at the end due to speed of calculation and it's accuracy.  Other models that we built were Regression Trees and Neural Network.  We also built these same models using Principle Component Analysis but the overall model fit wasn't as good as when using all the variables.  This is because PCA will reduce dimentionality but that is needed for classification problems.

**Coding Steps:**

1. Read and clean data
2. Partition the data in training and testing sets
3. Build a model with randomForest() function on training dataset
4. Evaluate out-of-sample Accuracy, i.e. cross-validation
3. Predict on final validation set


## R Code
### Data Reading & Cleaning
```{r SetParameters, message=FALSE}
# Load Libraries
library(caret)
library(randomForest)
library(corrplot)
```

```{r Read-Clean-Data, tidy=TRUE}
# Read and Clean Data
data       <- read.csv("~/GitHub/MachineLearning/Data/pml-testing.csv", na.string=c("NA", ""))
data.valid <- read.csv("~/GitHub/MachineLearning/Data/pml-training.csv", na.string=c("NA", ""))

# Determine Data's NA Columns
table(round(apply(data, 2, function(x) sum(is.na(x)))/nrow(data), 2))
columns.naCount <- apply(data, 2, function(x) sum(is.na(x)))

# Remove Data's NA Columns
columns.NA    <- names(columns.naCount)[columns.naCount> 0]
columns.notNA <- names(columns.naCount)[columns.naCount==0]
data <- data[, columns.notNA]
data <- data[, -c(1:7)]

# Set Seet
set.seed(3599)

# Partition Data
inTrain <- createDataPartition(y=data$classe, p=.75, list=F)
data.train <- data[ inTrain, ]
data.test  <- data[-inTrain, ]
```

### Modeling Building
```{r ModelBuilding, tidy=TRUE, cache=TRUE}
# Explore Data
cor.data <- cor(data[, -53])
corrplot(cor.data, method="color")

# Train Random Forest
modFit.RF <- randomForest(classe~., data=data.train)

# Compare Out-of-Sample Errors
percent <- function(x, digits = 2, format = "f", ...) {
    paste0(formatC(100 * x, format = format, digits = digits, ...), "%")
}

# Confusion Matrix
confMatrix <-confusionMatrix(data.test$classe, 
                             predict(modFit.RF, data.test))

confMatrix

# Out-of-Sample Error Rate
percent(1-confMatrix$overall[[1]])
```

### Final Prediction
```{r Predict, tidy=TRUE}
predict(modFit.RF, data.valid)
```